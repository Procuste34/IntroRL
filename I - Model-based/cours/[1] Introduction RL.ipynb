{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction au reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI, ML, ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'intelligence artificielle (AI) se caractérise par toutes sortes de méthodes tentant de rendre une machine intelligente. Mais que veut dire rendre une machine intelligente ? Voilà une définition un peu floue...\n",
    "\n",
    "Un sous-domaine de l'AI est le Machine Learning (apprentissage automatique), qui vise à donner la capacité aux machines d'apprendre à partir de données, leur permettant d'accomplir toutes sortes de tâches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ai.png\" width=\"780\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le ML comporte trois types de learning :\n",
    "* supervised learning : le programme doit dans ce cas-là apprendre une fonction qui, à partir d'une entrée, donne une sortie. Le programme reçoit pour cela un set d'entraînement comportant des couples (entrée; sortie). Une fois cette fonction apprise, il est alors possible de faire de l'inférence, c'est-à-dire prédire la sortie d'une entrée donnée. Ce champ est le plus répandu des trois dû à différents exploits récents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/supervised.png\" width=\"780\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">La tâche de classification d'images est un classique du supervised learning</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* unsupervised learning : le but du programme est ici de déterminer une structure dans des données. L'absence de signal de supervisement pour guider le learner distingue cette pratique des méthodes du supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/unsupervised.png\" width=\"780\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">Les Generative Adversarial Networks (GAN) sont capables de générer des images de personnes. Cette tâche fait partie du unsupervised learning : l'algorithme n'a eu besoin d'aucun signal de supervisement durant son entraînement.</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reinforcement learning : le learner est ici un agent, qui interagit avec un environnement en effectuant des actions. Les actions prises par l'agent changent le state de l'environnement et lui procurent un reward. Le learner ne reçoit pas un signal de supervisement lui disant quelles actions choisir à chaque instant : il n'apprend qu'à partir des rewards reçus, qu'il doit maximiser. Ces rewards, une fois maximisées, doivent permettre à l'agent de réussir la tâche souhaitée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cartpole.gif\" width=\"520\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">Un agent a ici appris à faire tenir en équilibre un bâton attaché sur un chariot. Il peut choisir entre déplacer le chariot à gauche, à droite, ou ne rien faire. L'agent observer à chaque timestep un state qui décrit la position et vélocité du bâton et du chariot. Enfin, le reward qu'il reçoit est proportionnel à l'angle du bâton : plus le bâton est incliné, plus le reward sera petit.</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reinforcement learning peut se diviser en deux parties :\n",
    "* model-based : l'agent, grâce à son expérience, établit un model de l'environnement grâce à des méthodes de <em>model learning</em>. Il peut ensuite utiliser des méthodes de <em>planning</em> qui, grâce au model appris, permettent de déterminer le comportement optimal.\n",
    "\n",
    "* model-free : l'agent ne connaît aucune dynamique ni reward de l'environnement. La tâche de learning ne se fait donc qu'à partir de son interaction directe avec l'environnement, l'agent apprend donc via du \"trial-and-error\". C'est une méthode bien plus applicable que le model based car elle peut fonctionner avec des problèmes où le nombre de states et d'actions sont infinis.\n",
    "\n",
    "Pour avoir une meilleure compréhension des algorithmes du model-free RL, il est toutefois important de maîtriser les algorithmes model based. C'est pourquoi ils seront d'abord étudiés ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, il ne s'agira ici que d'étudier des méthodes de <planning>. Ainsi, nous partirons de l'assomption que l'agent sait tout sur l'environnement. Il en connaît les dynamiques et peut ainsi prédire les states les plus probables. Il connaît également les rewards, et sait qu'en accomplissant telle action dans tel state lui rapportera tant de reward. La tâche de learning ici ne se fait donc qu'à partir de cette connaissance très précise de l'environnement. Deux problèmes majeurs de cette pratique : la connaissance complète de l'environnement peut être difficile à acquérir dans certains cas, comme lorsque le nombre de states possibles doit être fini, ce qui est très souvent le cas. Le second problème est comme dit précédemment, le fait que le nombre de states et le nombre d'actions possibles doit être fini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process - MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction agent-environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un Markov Decision Process est un framework très utilisé en reinforcement learning car il décrit formellement les interactions entre un agent et un environnement, ainsi que le problème de prise de décision séquentielle. Ainsi, si quelqu'un souhaite résoudre un problème de prise de décision séquentielle, il peut transofrmer ce problème en MDP, et ainsi bénéficier d'outils mais aussi de preuves que la solution optimale sera trouvée. \n",
    "\n",
    "* L'<em>agent</em> est abstrait et se caractérise seulement par son algorithme d'apprentissage et de prise de décision.\n",
    "\n",
    "* L'<em>environnement</em> représente tout ce qui est en dehors de l'agent.\n",
    "\n",
    "Ces deux entités intéragissent de la façon suivant :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/agentenv.png\" width=\"520\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chaque timestep $t$ :\n",
    "* l'agent reçoit le state de l'environnement $S_t$ et le reward $R_t$\n",
    "* l'agent effectue en conséquence une action $A_t$ sur l'environnement.\n",
    "\n",
    "Et à $t+1$, l'agent recevra le nouveau state $S_{t+1}$ ainsi qu'un reward $R_{t+1}$, qui dépendent de l'action $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi l'interaction agent-environnement génère une trajectoire :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large S_0, A_0, R_1, S_1, A_1, R_2, ...\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais cette interaction doit-elle se terminer ou peut elle continuer à l'infini ?\n",
    "\n",
    "Les deux cas sont possibles : une interaction qui a pour certitude de se terminer est qualifiée d'épisodique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $M = \\; <S, A, P, R>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un MDP se définit à partir d'un tuple $M = \\; <S, A, P, R>$ :\n",
    "* $S$ est l'ensemble des states dans lequel peut se trouver l'environnement.\n",
    "* $A$ est l'ensemble des actions que l'agent peut choisir.\n",
    "* $P$ représente les dynamiques de l'environnement, c'est-à-dire les probabilités de transition entre chaque state.\n",
    "* $R$ décrit les rewards obtenus en fonction d'un state et d'une action prise.\n",
    "\n",
    "$S$ et $A$ sont pour l'instant considérés comme finis.\n",
    "Les dynamiques de l'environnement $P$ ainsi que les rewards $R$ sont inconnus dans la partie model-free du reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamiques et propriété markovienne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dynamiques de l'environnement se caractérisent par une distribution de probabilité du state successeur $s'$ étant donné le state actuel $s$ et l'action prise $a$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large p(s'|s,a) = P[S_{t+1} = s' | S_t = s, A_t = a]\n",
    "\\\\\n",
    "\\large \\sum_{s'} p(s'|s,a) = 1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de constater que le state $s'$ <strong>ne dépend que du state $s$ et de l'action $a$</strong>, et non pas des anciens states. Cette assomption s'appelle la <em>propriété markovienne (Markov property)</em> et est nécessaire pour qu'un MDP soit applicable à un problème :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large P[S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1} ..., S_0, A_0] = P[S_{t+1} | S_t, A_t]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">Autrement dit, le futur est indépendent du passé, compte tenu du présent.</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup de problèmes respectent cette propriété. Et même si à première vu un problème ne semble pas le respecter, modifier l'espace des states (pour incorporer l'information manquante) permet de respecter cette propriété, même si cette modification entraînera son aggrandissement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'agent connaît donc $p(s'|s,a), \\forall s',s, a$, aussi écrit $P_{ss'}^a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em><span style=\"color:gray\">P peut être vu comme un tensor de dimensions $(|A| * |S| * |S|)$, soit $|A|$ matrices de $(|S| * |S|)$ décrivant les probabilités de transitions entre chaque state pour chaque action.</span></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les rewards de l'environnement sont définis pour chaque transition, et dépendent du state actuel $s$, de l'action prise $a$, et du state successeur $s'$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large R_{ss'}^a = E[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette défition générale du reward permet une grande fléxibilité car le reward peut ainsi dépendre de plus de facteurs. Cependant, pour rendre la suite plus intuitive, il est tout aussi raisonnable de penser que le reward reçu à la timestep $t+1$ ne dépend que du state $S_t$ et de l'action $A_t$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large R_{s}^a = E[R_{t+1} | S_t=s, A_t=a]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'agent connaît donc $R_s^a \\in R, \\forall s, a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em><span style=\"color:gray\">R peut être vu comme une matrice de dimensions $(|S| * |A|)$, soit |A| vecteurs décrivant les rewards reçus pur chaque state pour chaque action.</span></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résoudre un MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notion de return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résoudre un MDP revient à avoir un comportement qui maximise l'espérance du cumulative reward, aussi appelé return $G_t$, qui est la somme de tous les rewards que l'agent reçoit à partir de la timestep $t$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large G_t = R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le remarquer, cette définition de $G_t$ part du principe que l'interaction agent-environnement se termine à la timestep $T$, ce qui n'est pas toujours le cas. Cette défition pourrait être modifiée pour convenir à une interaction infinie, mais alors la variable aléatoire $G_t$ serait inifinie. C'est pourquoi la définition la plus courante de $G_t$ est la suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large G_t \\; & \\large = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\n",
    "\\\\\n",
    "& \\large = \\sum_{k=0}^\\infty \\gamma ^ k R_{t+k+1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma \\in [0;1]$ et est un hyperparamètre appelé facteur de réduction (discount factor), qui détermine l'importance des rewards les plus éloignés dans le temps. Ainsi, pour $\\gamma=0$, le return $G_t$ ne sera composé que de l'immediate reward $R_{t+1}$, ce qui fait que l'agent agira pour maximiser l'immediate reward seulement et oubliera les rewards futurs. L'agent est alors qualifiée de myope. Au contraire, si $\\gamma=1$, l'agent sera hypermétrope car les rewards futurs compteront autant que le reward immédiat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En plus d'assurer que le return $G_t$ ne soit pas infini, l'introduction du discounting s'inspire du comportement humain et animal qui privilégie les récompenses immédiates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em>\"Mieux vaut un que deux tu l'auras\"</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également important de noter pour la suite que le return $G_t$ peut être décomposé en deux parties : le reward immédiat $R_{t+1}$ et le reste des rewards futurs, $G_{t+1}$, discounted :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La policy, souvent dénoté $\\pi$, d'un agent peut s'apparenter à son comportement : compte tenu d'un state, elle dit à l'agent quelle action choisir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toute policy est stochastique, c'est-à-dire que pour tout state $s$, elle associe une probabilité à chaque action $a$: $\\pi(a|s)$. Ainsi :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas où la policy associe la probabilité de $1$ à une action $a$ et $0$ à toutes les autres pour tout state $s$, la policy est dite déterministique : $a = \\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de résoudre un MDP, il est utilise de définir la state-value function d'une policy $\\pi$, dénotée $v_\\pi$. Elle est définie pour tout state $s$ et est égale à l'espérance du return à partir du state $s$, si l'agent suit la policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large v_\\pi(s) = E_\\pi[G_t | S_t = s]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em><span style=\"color:gray\">\n",
    "Pourquoi la state-value function dépend d'une policy $\\pi$ ? \n",
    "</br>\n",
    "$v_\\pi$ est une manière d'évaluer $\\pi$, de savoir, en moyenne, le return qu'elle procure pour chaque state.\n",
    "</span></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même, on peut définir l'action-value function d'une policy $\\pi$, dénotée $q_\\pi$. Elle est définie pour tout state $s$ et action $a$, et est égale à l'espérance du return si l'agent exécute $a$ dans $s$ et suit ensuite $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large q_\\pi(s,a) = E_\\pi[G_t | S_t = s, A_t = a]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman expectation equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les équations de Bellman sont un outil très utilisé dans le reinforcement learning, à la fois model-based et model-free.\n",
    "\n",
    "Ici, les équations d'expectation de Bellman vont servir à approximer les fonctions $v_\\pi$ et $q_\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exprimer récursivement $v_\\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vu précédemment, $G_t = R_{t+1} + \\gamma G_{t+1}$. Ainsi, on peut écrire :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large v_\\pi(s) = E_\\pi[G_t | S_t = s] = E_\\pi[R_{t+1} + \\gamma G_t | S_t = s] = E_\\pi[R_{t+1} | S_t = s] + \\gamma E_\\pi[G_{t+1} | S_t = s]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{t+1}$ correspond au reward immédiat, et $G_{t+1}$ à la value du state successeur de $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer $E_\\pi[R_{t+1} | S_t = s]$ ainsi que $E_\\pi[G_{t+1} | S_t = s]$ permettra donc de calculer $v_\\pi(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_\\pi[R_{t+1} | S_t = s]$ représente le reward immédiat moyen que l'agent reçoit lorsqu'il sort du state $s$ et suit $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/expected_r_t+1.png\" width=\"300\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme montré dans le <em>backup diagram</em> ci-dessus, l'agent peut recevoir un parmis $|A|$ possibles immediate rewards en sortant du state $s$. Chaque reward $R_s^a$ a une probabilité $\\pi(a|s)$ d'être reçu par l'agent. Ainsi, l'espérance (ou moyenne) du reward immédiat $R_{t+1}$ du state $s$ se calcule :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large E_\\pi[R_{t+1} | S_t = s] =  \\sum_a \\pi(a|s) R_s^a\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_\\pi[G_{t+1} | S_t = s]$ représente le reward futur moyen que l'agent peut recevoir lorsqu'il est dans le state $s$, et suit $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/expected_g_t+1.png\" width=\"500\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le diagramme ci-dessus montre que, dans le state $s$, l'agent peut avoir différents futur return $G_{t+1}$, chacun associé à une probabilité d'arriver, tout comme l'agent pouvait précédemment recevoir plusieurs rewards immédiats $R_{t+1}$. La probabilité d'avoir un certain return $v_\\pi(s')$ est d'abord la probabilité $\\pi(a|s)$ de prendre l'action $a$ qui mène l'agent \"à portée\" de $s'$ multipliée par la probabilité $p(s'|s,a)$ de transitionner, compte tenu de $s$ et $a$, vers le state $s'$. Ainsi, la somme de ces probabilités avec leur valeur correspondante donne la définition suivante de $E_\\pi[G_{t+1}]$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large E_\\pi[G_{t+1} | S_t = s] = \\sum_a \\pi(a|s) \\sum_{s'} p(s'|s,a) v_\\pi(s')\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut donc écrire :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large v_\\pi(s) \\; & \\large = E_\\pi[R_{t+1} | S_t = s] + \\gamma E_\\pi[G_{t+1} | S_t = s]\n",
    "\\\\\n",
    "& \\large = \\sum_a \\pi(a|s) R_s^a + \\gamma \\sum_a \\pi(a|s) \\sum_{s'} p(s'|s,a) v_\\pi(s')\n",
    "\\\\\n",
    "& \\large = \\sum_a \\pi(a|s) [R_s^a + \\gamma \\sum_{s'} p(s'|s,a) v_\\pi(s')]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">Equation d'expectation de Bellman pour $v_\\pi(s)$</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exprimer récursivement $q_\\pi(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vu précédemment, $G_t = R_{t+1} + \\gamma G_{t+1}$. Ainsi, on peut écrire : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large q_\\pi(s,a) = E_\\pi[G_t | S_t = s, A_t = a] = E_\\pi[R_{t+1} + \\gamma G_t | S_t = s, A_t = a] = E[R_{t+1} | S_t = s, A_t =a ] + \\gamma E_\\pi[G_{t+1} | S_t = s, A_t = a]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer $E[R_{t+1} | S_t=s, A_t=a]$ ainsi que $E_\\pi[G_{t+1} | S_t=s, A_t=a]$ permettra donc de calculer $q_\\pi(s, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E[R_{t+1} | S_t=s, A_t=a]$ représente l'espérance du reward immédiat étant donné le current state $s$ et l'action prise $a$. Cette valeur est fournie à l'agent par l'environnement : c'est $R_s^a$. Aucun calcul n'est donc nécessaire car l'agent sait qu'il va prendre l'action $a$ avec une probabilité de $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/q_expected_r_t+1.png\" width=\"300\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_\\pi[G_{t+1} | S_t=s, A_t=a]$ représente le reward futur moyen que l'agent peut recevoir lorsqu'il est dans le state $s$, prend l'action $a$, et suit ensuite $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/q_expected_g_t+1.png\" width=\"500\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le raisonnement pour calculer $E_\\pi[G_{t+1} | S_t=s, A_t=a]$ est similaire à celui utilisé avec $v_\\pi(s)$ mais est même plus simple car l'agent sait quelle action $a$ il va prendre. Le diagramme montre donc que l'agent peut avoir différents futur return $G_{t+1}$ chacun associé à une probabilité d'arriver. La probabilité d'avoir un certain futur return $v_\\pi(s')$ est égale à la probabilité $p(s'|s,a)$ de transitionner, compte tenu de $s$ et $a$, vers le state $s'$. Ainsi : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large E_\\pi[G_{t+1} | S_t = s, A_t = a] = 1 \\sum_{s'} p(s'|s,a) v_\\pi(s')\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large q_\\pi(s,a) & \\large = E[R_{t+1} | S_t = s | A_t = a] + \\gamma E_\\pi[G_{t+1} | S_t = s, A_t = a]\n",
    "\\\\\n",
    "&\n",
    "\\large \\; = R_s^a + \\gamma \\sum_{s'} p(s'|s,a) v_\\pi(s')\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimalité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\pi_*$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La policy optimale $\\pi_*$ est par définition la policy qui, pour tout state $s$ et pour toute policy $\\pi$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large v_{\\pi_*}(s) \\geq v_\\pi(s), \\quad \\forall \\pi,s\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est très souvent possible qu'il existe une multitude de policy optimale, voir même une infinité de policy optimales stochastiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $v_*$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La state-value function optimale $v_*$ à la propriété suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large v_*(s) = v_{\\pi_*}(s) = max_\\pi v_\\pi(s), \\quad \\forall s\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'existe qu'une seule statue-value optimale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $q_*$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La action-value function optimale $q_*$ à la propriété suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large q_*(s,a) = q_{\\pi_*}(s,a) = max_\\pi q_\\pi(s,a), \\quad \\forall s,a\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:red\">Attention : $a$ n'est pas forcément l'action optimale à prendre dans le state $s$</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exprimer récursivement $v_*$ et $q_(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/backup_v_star.png\" width=\"500\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le trait rouge présent sur le diagramme signifie que $v_*(s)$ prendra la valeur la plus grande entre les deux branches. Cela se traduit sous forme d'équation ainsi :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large v_*(s) = max_a [R_s^a + \\gamma \\sum_{s'} p(s'|s,a) v_*(s')]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, si $v_*$ est calculée, la valeur de tout state $s$ sera la maximale car $v_*(s)$ utilise l'action optimale du state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/backup_q_star.png\" width=\"1000\" height=\"50\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le principe s'applique de la même manière avec $q_*(s,a)$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large q_*(s,a) & \\large = R_s^a + \\gamma \\sum_{s'} p(s'|s,a) v_*(s')\n",
    "\\\\\n",
    "&\n",
    "\\large \\; = R_s^a + \\gamma \\sum_{s'} p(s'|s,a) max_{a'} q_*(s',a')\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le max est ici pris lorsque l'agent prend l'action $a'$ qui est l'action optimale dans le state $s'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la state-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le MDP est considéré comme résolu lorsque la state-value function $v_*$ est trouvée. Il existe une méthode appelée programmation dynamique, introduite par Bellman, qui est très utilisée pour résoudre un MDP. Généralement, le dynamic programming vise à résoudre un problème et pour cela, le décompose en plusieurs sub-problems, qu'il résout chacun d'un côté. La même idée s'applique au MDP : trouver un comportement optimal revient à d'abord trouver la meilleure action optimale à prendre maintenant puis toutes les suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de <strong>Policy Evaluation</strong> vise à calculer $v_\\pi$ et s'appuie pour cela sur l'équation d'expectation de Bellman.\n",
    "</br>L'algorithme de <strong> Policy Improvement</strong> vise à améliorer une policy à partir de sa value function, il s'appuie sur l'équation d'optimality de Bellman.\n",
    "</br>Enfin l'algorithme de <strong>Value Iteration</strong> est une application itérative de l'équation d'optimality de Bellman et calcule directement $v_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Reinforcement Learning implique donc l'interaction d'un agent (le learner) et d'un environnement. L'agent effectue des actions, voit le state de l'environnement changer et reçoit des rewards, qu'il tente de maximiser. L'agent peut tout connaître de son environnement, ainsi la tâche est appelée model-based learning, et model-free learning dans le cas contraire. Nous nous intéressons pour commencer au cas du model-based learning, en utilisant des tableaux pour les fonctions $\\pi, v, q$.\n",
    "</br>\n",
    "Un MDP est une définition formelle de l'interaction agent-environnement, qui permet d'assurer la convergence sur la solution optimale. Il est définit, dans le cas du model-based learning, à partir d'un state space qui possèdent la propriété markovienne, un action space, des dynamiques et des rewards. Le comportement de l'agent est définit par sa policy $\\pi$, qui est évaluée grâce à une state-value function $v_\\pi$ ou encore grâce à une action-value function $q_\\pi$.\n",
    "</br>\n",
    "Les équations de Bellman proposent une définition récursive de $v_\\pi$, $q_\\pi$, $v_*$ et $q_*$, qui servent de base pour calculer ces fonctions et ainsi résoudre le MDP. Il s'agit seulement d'un développement d'une espérance, donc on fait la somme de toutes les valuers possibles, pondérées par leur probabilité d'arriver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
