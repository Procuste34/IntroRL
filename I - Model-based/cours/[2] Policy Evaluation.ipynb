{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de policy evaluation consiste à calculer la value function d'une policy donnée pour chaque state $s$. Plus formellement, il s'agit d'une application iterative de l'équation d'expectation de Bellman, montrée ci-dessous. Cette équation donne une définition de $v_{π}(s)$, l'expected return à compter du state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large v_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ v_\\pi(s')]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">Bellman's expectation equation</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mise à jour de la value function $V$ se fait de façon asynchrone (voir `Notes`) de la manière suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large V(s) \\; & \\large \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ V(s')]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le pseudocode de l'algorithme :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pseudocode_policyevaluation.png\" width=\"600\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">Sutton and Barto, chapitre 4</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'environnement utilisé ici est tiré du chapitre 4 du livre (exemple 4.1). Il s'agit d'un gridworld dans lequel l'agent peut exécuter 4 actions déterministiques : haut, droite, bas, gauche. Quelque soit le state de l'agent, celui recevra un reward de -1 après avoir exécuter n'importe quelle action. Toutefois, si l'agent atteint les cases 0 ou 15, l'épisode est terminé. Ainsi, si un agent veut maximiser les rewards obtenus au cours d'un épisode, il doit aller au plus vite sur la case 0 ou 15. Le state de départ de l'agent est random, bien qu'il ne peut pas être 0 ni 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gridworld_states.png\" width=\"250\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import envs.gridworld_dennybritz as grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grd.GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette fonction sert d'interface avec l'environnement.\n",
    "* `compute_q_value_for_s_a(env, V, s, a, gamma)` retourne, pour le state $s$ et l'action $a$ spécifiés la valeur $Q(s,a)$ soit $\\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]$. <br>Cette fonction interroge la fonction `P[s][a]` de l'environnement qui renvoit une liste de tuple de la forme : `(p(s'|s,a), s', r(s,a,s'), done?)`. L'algorithme loop sur ces tuples et ajoute à `q` (équivalent à $Q(s,a)$) la valeur $p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value_for_s_a(env, V, s, a, gamma):\n",
    "    q = 0\n",
    "    \n",
    "    for p_sPrime, sPrime, r_ss_a, done in env.P[s][a]:\n",
    "        q += p_sPrime * (r_ss_a + gamma * V[sPrime])\n",
    "        \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La policy $\\pi$ à évaluer est initalizée, donnant à chacune des 4 actions une probabilité de $0.25$ à être exécutée, dans tout state $s$. $\\pi$ est ici une table de dimensions $|S|x|A|$. Ainsi, la probabilité d'exécuter $a$ dans le state $s$ est stored dans `pi[s][a]`.\n",
    "\n",
    "La value function $V$ est initalizée randomly, ici elle attribue une valeur de 0 à tous les states.\n",
    "\n",
    "Les hyperparamètres $\\gamma$ and $\\theta$ sont également initializés. Le discount factor prend la valeur de 1, cela ne posera pas de problème car l'horizon est dans ce cas là fini. Le threshold $\\theta$ détermine l'écart maximal entre deux mises à jour de value function à partir duquel l'algorithme aura considéré qu'il a convergé sur $v_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.ones([env.nS, env.nA]) * 0.25\n",
    "\n",
    "V = np.zeros([env.nS, 1])\n",
    "\n",
    "gamma = 1.0\n",
    "theta = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque iteration, pour chaque state $s$, l'algorithme doit mettre à jour $V(s)$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large V(s) \\; & \\large \\leftarrow \\sum_{a} \\pi(a|s) Q(s, a)\n",
    "\\\\\n",
    "& \\large \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, pour chaque state $s$, on doit calculer pour chaque action $a$ la valeur $Q(s, a)$ renvoyée par la fonction `compute_q_value_for_s_a`. \n",
    "\n",
    "$Q(s,a)$ sera ensuite ajouté à la nouvelle valeur $V(s)$, mais sera d'abord multiplié par la probabilité d'arriver $\\pi(a|s)$, `pi[s][a]`.\n",
    "\n",
    "L'algorithme est automatiquement arrêté lorsque l'écart maximum entre $V_{k+1}$ et $V_k$ atteint $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 141 iterations\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "while True:\n",
    "    k+=1\n",
    "    delta = 0\n",
    "    \n",
    "    for s in range(env.nS):\n",
    "        V_new = 0\n",
    "        \n",
    "        for a in range(env.nA):\n",
    "            prob_a = pi[s][a]\n",
    "            q_s_a = compute_q_value_for_s_a(env, V, s, a, gamma)\n",
    "            \n",
    "            V_new += prob_a * q_s_a\n",
    "        \n",
    "        delta = max(delta, np.abs(V_new - V[s]))\n",
    "        V[s] = V_new\n",
    "\n",
    "    if(delta < theta) :\n",
    "        print(\"Finished after \" + str(k) + \" iterations\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La value function obtenue est affichée ci-dessous.\n",
    "Les states les plus proches des cases 0 et 15 ont bien la plus grande value. Comme le reward est -1 pour chaque action, on peut interpréter ces résultats comme étant le nombre de steps moyen avant d'atteindre le state terminal, en suivant la policy random $\\pi$ donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gridworld_states_values.png\" width=\"250\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ],\n",
       "       [-13.99993529],\n",
       "       [-19.99990698],\n",
       "       [-21.99989761],\n",
       "       [-13.99993529],\n",
       "       [-17.9999206 ],\n",
       "       [-19.99991379],\n",
       "       [-19.99991477],\n",
       "       [-19.99990698],\n",
       "       [-19.99991379],\n",
       "       [-17.99992725],\n",
       "       [-13.99994569],\n",
       "       [-21.99989761],\n",
       "       [-19.99991477],\n",
       "       [-13.99994569],\n",
       "       [  0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
