{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas spécial du MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de Value Iteration calcule directement $v_*$, sans utiliser de policy. Il reprend en réalité la même structure que le Policy Iteration, mais ne met à jour qu'une seule fois $V(s), \\forall s$ pour chaque évaluation. En effet il s'agit d'un cas spécial du Modified Policy Iteration, où $k=1$. Le pseudocode est alors le suivant : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "repeat, while $V$ stops updating:\n",
    "\n",
    "&emsp;<strong>Improvement</strong><br/>\n",
    "&emsp;for each $s$:<br/>\n",
    "&emsp;&emsp;&emsp; $\\pi(s) \\leftarrow argmax_a [\\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ V(s')]]$\n",
    "\n",
    "&emsp;<strong>Evaluation</strong><br/>\n",
    "&emsp; for each $s$:<br/>\n",
    "&emsp;&emsp;&emsp; $V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ V(s')]$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em><span style=\"color:gray\">L'improvement et l'évaluation ont ici été échangées de place, cela n'a aucune conséquence sur le résultat du programme car ces deux parties sont exécutées en boucle.</span></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dénotons $a_*$ l'action optimale dans le state $s$. </br>\n",
    "$\\pi(s) = argmax_a [\\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]] = argmax_a[Q(s,a)] = a_*$, donc </br>\n",
    "$V(s) = max_a [\\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]] = max_a [Q(s,a)] = \\sum_{s'} p(s'|s,a_*) [R_{ss'}^{a_*} + \\gamma V(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, on peut réécrire le pseudocode comme suit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "repeat, while $V$ stops updating:\n",
    "\n",
    "&emsp; for each $s$:<br/>\n",
    "&emsp;&emsp; $V(s) \\leftarrow max_a [\\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]]$\n",
    "</br>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est intéressant de remarquer que la règle de mise à jour de $V(s)$ est en réalité une application itérative de la Bellman optimality equation, ce qui fait qu'il calcule directement $v_*$. De plus, l'algorithme n'improve plus explicitement la policy $\\pi$ car le calcul de $V(s)$ ne nécessite plus les probabilités d'actions choisies par une policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import envs.gridworld_dennybritz as grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grd.GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette fonction sert d'interface avec l'environnement.\n",
    "* `compute_q_value_for_s_a(env, V, s, a, gamma)` retourne, pour le state $s$ et l'action $a$ spécifiés la valeur $Q(s,a)$ soit $\\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]$. <br>Cette fonction interroge la fonction `P[s][a]` de l'environnement qui renvoit une liste de tuple de la forme : `(p(s'|s,a), s', r(s,a,s'), done?)`. L'algorithme loop sur ces tuples et ajoute à `q` (équivalent à $Q(s,a)$) la valeur $p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value_for_s_a(env, V, s, a, gamma):\n",
    "    q = 0\n",
    "    \n",
    "    for p_sPrime, sPrime, r_ss_a, done in env.P[s][a]:\n",
    "        q += p_sPrime * (r_ss_a + gamma * V[sPrime])\n",
    "        \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La value function $V$ est initializée à des valeurs random.\n",
    "\n",
    "Les hyperparamètres $\\gamma$ and $\\theta$ sont également initializés. Le discount factor prend la valeur de 1, cela ne posera pas de problème car l'horizon est dans ce cas là fini. Le threshold $\\theta$ détermine l'écart maximal entre deux mises à jour de la value function à partir duquel l'algorithme aura considéré qu'il a convergé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.zeros([env.nS, 1])\n",
    "\n",
    "gamma = 1.0\n",
    "theta = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque itération, pour chaque state $s$, l'algorithme calcule les values de toutes les actions, $Q(s,a), \\forall a$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large Q(s,a) \\large \\leftarrow \\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce calcul est réalisé par la fonction `compute_q_value_for_s_a`. Toutes les valeurs $Q(s,a)$ sont stockées dans le tableau `q_s`, de dimensions $|A|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois $Q(s,a)$ calculé pour toutes les actions, $V(s)$ prend la valeur maximale de ce tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 4 iterations.\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "while True:\n",
    "    k+=1\n",
    "    delta = 0\n",
    "    \n",
    "    for s in range(env.nS):\n",
    "        q_s = np.zeros([env.nA, 1])\n",
    "        \n",
    "        for a in range(env.nA):\n",
    "            q_s[a] = compute_q_value_for_s_a(env, V, s, a, gamma)\n",
    "\n",
    "        newV = np.max(q_s)\n",
    "        delta = max(delta, np.abs(newV - V[s]))\n",
    "        V[s] = newV\n",
    "        \n",
    "    if(delta < theta):\n",
    "        print(\"Finished after \" + str(k) + \" iterations.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [-1.],\n",
       "       [-2.],\n",
       "       [-3.],\n",
       "       [-1.],\n",
       "       [-2.],\n",
       "       [-3.],\n",
       "       [-2.],\n",
       "       [-2.],\n",
       "       [-3.],\n",
       "       [-2.],\n",
       "       [-1.],\n",
       "       [-3.],\n",
       "       [-2.],\n",
       "       [-1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La value function $v_*$ a été trouvée :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gridworld_states_values_best.png\" width=\"250\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La policy $\\pi_*$ peut ainsi être facilement déduite de $v_*$, grâce à l'algorithme de policy improvement par exemple."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
