{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[EXPLIQUER NOTATION, avec notamment le underscore sur les states et actions pour signifier une timestep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[$v_\\pi$ VS $V$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous VS Asynchronous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les algorithmes de policy evaluation et de value iteration montrés font des mises à jour <em>asynchronous</em>, ce qui signifie qu'une seule liste est utilisée pour stockée les valeurs de l'itération $k-1$ et celles de l'itération $k$. Cela implique que la mise à jour du state $s$ peut être faite avec des values d'autres states déjà mises à jour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Synchronous</strong> : $V_{k+1}(s) \\leftarrow \\sum_{a} \\pi(a|s) [R_s^a + γ \\sum_{s'} p(s'|s,a) V_k(s')]$ <em><span style=\"color:gray\">Deux arrays utilisés ici : $V_{k+1}$ et $V_k$.</span></em>\n",
    "</br><strong>Asynchronous</strong> : $V(s) \\leftarrow \\sum_{a} \\pi(a|s) [R_s^a + γ \\sum_{s'} p(s'|s,a) V(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards dépendant du state successeur $s'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précisé dans le notebook d'introduction, le reward reçu au timestep $t+1$ peut dépendre, en plus du state $S_{t}$ et de l'action prise $A_{t}$, du state successeur $S_{t+1}$. Il est alors noté $R_{ss'}^a$. L'équation d'expectation de Bellman pour $v_\\pi(s)$ est alors la suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\large v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma v_\\pi(s')]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est cette forme de la Bellman qui est choisie pour l'implementation Python des algorithmes tout simplement car les rewards donnés par les environnements Gym dépendent sur state successeur $s'$ ( donc $R_{ss'}^a$). Toutefois, une fois l'équation de Bellman comportant $R_s^a$ comprise, il est très facile de comprendre celle présentée au dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les rewards sont designés par le créateur de l'environnement et, une fois maximisés, doit permettre à l'agent d'atteindre le but souhaité. Cependant, il est très important que les rewards disent à l'agent quel but atteindre plutôt que comment l'atteindre. Cela l'empêchera tout d'abord de trouver un raccourci non souhaité qui lui ferait maximizer les rewards mais sans attreindre le but souhaité. De plus, cela le limiterai dans son comportement et ne pourrait pas trouver des comportements meilleurs que ceux pensés par les designer de la fonction de reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méthode tabulaire VS méthode d'approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode tabulaire considère les policy, state-value function et action-value function comme des tableaux. Ainsi, une mise à jour complète de $v_\\pi$ par exemple nécessite de regarder tous les states. Cette méthode ne s'applique donc pas aux problèmes comportant un state space très grand voir infini. La méthode d'approximation permet de gérer ces problèmes en utilisant des approximateurs de fonctions pour calculer les policy et value functions.\n",
    "\n",
    "La méthode tabulaire s'applique parfaitement au model-based learning car il nécessite déjà un state space fini. Elle peut également s'appliquer au model-free learning dans le cas où le state space du problème est fini. Dans le cas contraire, une méthode d'approximation est mieux adaptée.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types d'environnement utilisés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les environnements compatibles de base avec les algorithmes du dossier `algos` sont des environnements de type Gym, qui possèdent une fonction `P[s][a]`, qui renvoit une liste de tuple de la forme : `(p(s'|s,a), s', r(s,a,s'), done?)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 6, -1.0, False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import envs.gridworld_dennybritz as grd\n",
    "env = grd.GridworldEnv()\n",
    "\n",
    "env.P[5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-761ee32e76ca>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-761ee32e76ca>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [LIBRAIRIE GYM]\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[LIBRAIRIE GYM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
