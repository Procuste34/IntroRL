{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement : $\\pi = greedy(V)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notion d'improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir une meilleure intuition à propos de cet algorithme, il est d'abord intéressant de comparer les policies entre elles. Comment, à partir de deux policies, déterminer la meilleure ?\n",
    "\n",
    "Il faut pour cela définir une manière de les classer. Soit deux policies déterministiques $\\pi$ et $\\pi'$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si &emsp; &ensp; $q_\\pi(s, \\pi'(s)) \\geq q_\\pi(s, \\pi(s))$ &ensp; $\\forall s$, </br>\n",
    "alors &ensp; $v_{\\pi'}(s) \\geq v_\\pi(s)$ &ensp; $\\forall s$ </br> \n",
    "&ensp; &ensp; &ensp; &ensp; $\\pi' \\geq \\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La condition $q_\\pi(s, \\pi'(s)) \\geq q_\\pi(s, \\pi(s))$ $\\forall s$ requiert que, pour tout state $s$, suivre $\\pi'$ pour une timestep et ensuite suivre $\\pi$ rapporte au moins autant, voir plus, de cumulative reward que suivre $\\pi$ dès le state $s$. Si cette condition est respectée, alors il est raisonnable de penser que $\\pi'$ est, au pire, aussi bonne que $\\pi$. Cela se traduit par l'implication $v_{\\pi'}(s) \\geq v_\\pi(s) \\forall s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, définissons une policy déterministique $\\pi$ , que nous allons improve. Pour cela, attribuons à la policy $\\pi'$ l'action $a$ qui maximise $q_\\pi(s,a)$, pour tout state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large \\pi'(s) \\large \\leftarrow argmax_a \\; q_\\pi(s, a) \\quad \\forall s\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, pour chaque state $s$, la policy $\\pi'$ se voit attribuer la meilleure action à exécuter. Si $\\pi'(s) = \\pi(s)$, alors il n'y a pas eu d'improvement car l'action attribuée à $\\pi'$ était déjà celle de $\\pi$. Si toutefois $\\pi'(s) \\ne \\pi(s)$, alors l'action attribuée à $\\pi'$ est par définition meilleure (i.e. l'expected cumulative reward sera plus grand) que l'action $\\pi(s)$.\n",
    "\n",
    "Ainsi, on peut affirmer que:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large q_\\pi(s, \\pi(s')) \\large \\geq q_\\pi(s, \\pi(s)) \\quad \\forall s\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela prouve donc que la policy $\\pi'$ est au pire aussi bonne que $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les policies utilisées ici pour démontrer l'improvement sont déterministiques pour rendre la preuve plus simple, mais cela s'applique également aux policies stochastiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de policy improvement consiste à améliorer une policy $\\pi$ par rapport à sa state-value function $Q$, pour chaque state $s$. Il est souvent dit de cet improvement que la policy agit \"greedily\" par rapport à la state-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'improvement assigne à $\\pi$ la meilleure action $a$ pour chaque state $s$. Chaque action est évaluée grâce à la value function $V$, connue. (calculée par l'algorithme de policy evaluation)\n",
    "\n",
    "Il peut ainsi être décrit comme suit pour une policy $\\pi$ déterministique:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large \\pi(s) \\; & \\large \\leftarrow argmax_a \\; Q(s, a) \\quad \\forall s\n",
    "\\\\\n",
    "& \\large \\leftarrow argmax_a \\; [\\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ  V(s')]] \\quad \\forall s\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, pour chaque state $s$, l'algorithme devra calculer la value $\\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ  v_\\pi(s')]$ de chaque action $a$, et assignera à la policy $\\pi$ l'action qui a la plus grande value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import envs.gridworld_dennybritz as grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grd.GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette fonction sert d'interface avec l'environnement.\n",
    "* `compute_q_value_for_s_a(env, V, s, a, gamma)` retourne, pour le state $s$ et l'action $a$ spécifiés la valeur $Q(s,a)$ soit $\\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]$. <br>Cette fonction interroge la fonction `P[s][a]` de l'environnement qui renvoit une liste de tuple de la forme : `(p(s'|s,a), s', r(s,a,s'), done?)`. L'algorithme loop sur ces tuples et ajoute à `q` (équivalent à $Q(s,a)$) la valeur $p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value_for_s_a(env, V, s, a, gamma):\n",
    "    q = 0\n",
    "    \n",
    "    for p_sPrime, sPrime, r_ss_a, done in env.P[s][a]:\n",
    "        q += p_sPrime * (r_ss_a + gamma * V[sPrime])\n",
    "        \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La policy $\\pi$ à improve est initalizée, donnant à chacune des 4 actions une probabilité de $0.25$ à être exécutée, dans tout state $s$. $\\pi$ est ici une table de dimensions $|S|x|A|$. Ainsi, la probabilité d'exécuter $a$ dans le state $s$ est stored dans `pi[s][a]`.\n",
    "\n",
    "La value function $V$ est celle calculée par l'algorithme d'évaluation.\n",
    "\n",
    "L'hyperparamètre $\\gamma$ est également initializé. Il prend la valeur de 1, cela ne posera pas de problème car l'horizon est dans ce cas là fini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.ones([env.nS, env.nA]) * 0.25\n",
    "gamma = 1\n",
    "\n",
    "V = np.array([[0], [-14], [-20], [-22], [-14], [-18], [-20], [-20], [-20], [-20], [-18], [-14], [-22], [-20], [-14], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque state $s$, les entrées de la table `q_s` de dimensions $|A|x|1|$ sont toutes initializées à 0. L'algorithme doit alors remplir cette table avec la value $Q(s,a)$ = $\\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ V(s')]$ pour chaque action $a$.\n",
    "\n",
    "Ce calcul est réalisé par la fonction `compute_q_value_for_s_a`.\n",
    "\n",
    "Une fois $Q(s, a)$ calculé pour tout $a$, l'algorithme assigne une probabilité de $1$ à l'action $a$ qui maximize $Q(s, a)$, et une probabilité de $0$ à toutes les autres actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(env.nS):\n",
    "    q_s = np.zeros([env.nA, 1])\n",
    "    \n",
    "    for a in range(env.nA):        \n",
    "        q_s[a] = compute_q_value_for_s_a(env, V, s, a, gamma)\n",
    "    \n",
    "    best_a = np.argmax(q_s)\n",
    "    pi[s] = np.eye(env.nA)[best_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La policy $\\pi$ obtenue est représenté dans le gridworld ci-dessous, chaque flèche correspondant à l'action choisie par $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gridworld_actions.png\" width=\"250\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas-là, il est facile de déduire cette policy depuis la value function fournie, sans appliquer l'algorithme décrit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gridworld_states_values.png\" width=\"250\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il se trouve que dans ce cas-là, il n'a suffit que d'une évaluation ($V$) et d'un improvement pour que la policy $\\pi$ trouvée soit optimale, elle est donc égale à $\\pi^*$. Dans des cas plus complexes, avec notamment plus de states, il faut plusieurs évaluations et improvements pour trouver $\\pi^*$, c'est le principe même de l'algorithme policy iteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
