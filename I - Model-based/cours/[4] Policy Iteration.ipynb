{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Policy Iteration est l'un des deux algorithmes les plus utilisés pour résoudre un MDP dont les dynamiques ($p(s'|s,a$)) et les rewards ($R_s^a$) sont connus. </br>\n",
    "L'algorithme de policy iteration alterne autant de fois que nécessaire l'évaluation et l'improvement d'une policy $\\pi$, afin de trouver la solution du MDP $\\pi_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/schema_policyiteration.png\" width=\"250\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">Sutton and Barto, chapitre 4</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'évaluation consiste à calculer la value fonction $V$ de la policy $\\pi$ pour tous les states $s$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large V(s) \\; & \\large \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ V(s')] \\quad \\forall s\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'improvement assigne à la policy $\\pi$ pour chaque state $s$ la meilleure action par rapport à sa value function $V$. Autrement dit, elle assigne l'action $a$ qui maximize $[R_s^a + γ \\sum_{s'} p(s'|s,a) V(s')]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\large \\pi(s) \\; & \\large \\leftarrow argmax_a \\; Q(s, a) \\quad \\forall s\n",
    "\\\\\n",
    "& \\large \\leftarrow argmax_a \\; [\\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ V(s')]] \\quad \\forall s\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de policy iteration consiste à alterner entre l'évaluation et l'improvement jusqu'à convergence : \n",
    "* <strong>Evaluation</strong> : calculer `V_new`. Si la nouvelle évaluation `V_new` est suffisament proche (déterminé par $\\theta$) de la précédente `V`, l'aglorithme s'arrête\n",
    "* <strong>Improvement</strong> : $\\pi = greedy(V)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pseudocode de l'algorithme est montré ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pseudocode_policyiteration.png\" width=\"600\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">Sutton and Barto, chapitre 4</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><em><span style=\"color:gray\">Ici, l'algorithme s'arrête si la nouvelle policy et la précédente sont égales. Cependant, il est possible qu'il existe plusieurs policy optimales $\\pi_*$ , ce qui rendrait cet algorithme bloqué entre deux ou plus policy optimales. Il ne peut y avoir par contre qu'une seule value function $v_*$.</span></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import envs.gridworld_dennybritz as grd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grd.GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cette fonction sert d'interface avec l'environnement.\n",
    "* `compute_q_value_for_s_a(env, V, s, a, gamma)` retourne, pour le state $s$ et l'action $a$ spécifiés la valeur $Q(s,a)$ soit $\\sum_{s'} p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]$. <br>Cette fonction interroge la fonction `P[s][a]` de l'environnement qui renvoit une liste de tuple de la forme : `(p(s'|s,a), s', r(s,a,s'), done?)`. L'algorithme loop sur ces tuples et ajoute à `q` (équivalent à $Q(s,a)$) la valeur $p(s'|s,a) [R_{ss'}^a + \\gamma V(s')]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value_for_s_a(env, V, s, a, gamma):\n",
    "    q = 0\n",
    "    \n",
    "    for p_sPrime, sPrime, r_ss_a, done in env.P[s][a]:\n",
    "        q += p_sPrime * (r_ss_a + gamma * V[sPrime])\n",
    "        \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and improvement functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ces deux fonctions servent respectivement d'évaluer une policy et de l'improve.\n",
    "* `evaluate_policy(env, pi, V, gamma, theta)` reprend la même structure que vu précédemment. Elle retourne en plus le boolean `improved`, qui détermine si il y a eu en changement entre la value function originale et la nouvelle.\n",
    "* `improve_policy(env, pi, V, gamma)` retourne la policy improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, pi, V, gamma, theta):\n",
    "    #env : OpenAI-like env\n",
    "    #pi : [nS * nA] matrix giving for each state a probability distribution over all actions\n",
    "    #V : [nS * 1] column vector of state values\n",
    "    #gamma : discount factor, e [0, 1]\n",
    "    #theta : threshold\n",
    "    \n",
    "    #returns V : [nS * 1] column vector of updates state values\n",
    "    #        improved : whether or not V has been improved\n",
    "    \n",
    "    V_updated = np.copy(V)\n",
    "    improved = True\n",
    "    \n",
    "    while True:        \n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            V_new = 0\n",
    "        \n",
    "            for a in range(env.nA):\n",
    "                prob_a = pi[s][a]\n",
    "                q_s_a = compute_q_value_for_s_a(env, V, s, a, gamma)\n",
    "            \n",
    "                V_new += prob_a * q_s_a\n",
    "        \n",
    "            delta = max(delta, np.abs(V_new - V_updated[s]))\n",
    "            V_updated[s] = V_new\n",
    "\n",
    "        if(delta < theta):\n",
    "            break\n",
    "    \n",
    "    if(np.array_equal(V, V_updated)):\n",
    "        improved = False\n",
    "    \n",
    "    return V_updated, improved\n",
    "\n",
    "def improve_policy(env, pi, V, gamma):\n",
    "    #env : OpenAI-like env\n",
    "    #pi : [nS * nA] matrix giving for each state a probability distribution over all actions\n",
    "    #V : [nS * 1] column vector of state values\n",
    "    #gamma : discount factor, e [0, 1]\n",
    "    #theta : threshold\n",
    "    \n",
    "    #returns pi : updated pi\n",
    "    \n",
    "    for s in range(env.nS):\n",
    "        q_s = np.zeros([env.nA, 1])\n",
    "    \n",
    "        for a in range(env.nA):\n",
    "            q_s[a] = compute_q_value_for_s_a(env, V, s, a, gamma)\n",
    "        \n",
    "        best_a = np.argmax(q_s)\n",
    "        pi[s] = np.eye(env.nA)[best_a]\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La policy $\\pi$ et la value function $V$ sont initalizées à des valeurs random.\n",
    "\n",
    "Les hyperparamètres $\\gamma$ and $\\theta$ sont également initializés. Le discount factor prend la valeur de 1, cela ne posera pas de problème car l'horizon est dans ce cas là fini. Le threshold $\\theta$ détermine l'écart maximal entre deux mises à jour de value function à partir duquel l'algorithme aura considéré qu'il a convergé sur $v_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.ones([env.nS, env.nA]) * 0.25\n",
    "V = np.zeros([env.nS, 1])\n",
    "\n",
    "gamma = 1.0\n",
    "theta = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chaque iteration $k$, l'algorithme performe une évaluation et un improvement de $\\pi$. Dès que la value function a convergé, l'algorithme s'arrête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 4 iterations.\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "while True:\n",
    "    k+=1\n",
    "\n",
    "    V, improved = evaluate_policy(env, pi, V, gamma, theta)\n",
    "    pi = improve_policy(env, pi, V, gamma)\n",
    "    \n",
    "    if(improved == False):\n",
    "        print(\"Finished after \" + str(k) + \" iterations.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.],\n",
       "        [-1.],\n",
       "        [-2.],\n",
       "        [-3.],\n",
       "        [-1.],\n",
       "        [-2.],\n",
       "        [-3.],\n",
       "        [-2.],\n",
       "        [-2.],\n",
       "        [-3.],\n",
       "        [-2.],\n",
       "        [-1.],\n",
       "        [-3.],\n",
       "        [-2.],\n",
       "        [-1.],\n",
       "        [ 0.]]), array([[1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La policy $\\pi_*$ ont été trouvé par l'algorithme :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gridworld_actions.png\" width=\"250\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi que que la value function $v_*$ qui, de par la nature du problème, représente le nombre de steps moyen avant la fin de l'épisode, en suivant la policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gridworld_states_values_best.png\" width=\"250\" height=\"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn more:\n",
    "* http://incompleteideas.net/book/first/ebook/node46.html\n",
    "* http://billy-inn.github.io/blog/2016/10/06/notes-on-reinforcement-learning-2-dynamic-programming/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# + Modified Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de policy iteration a-t-il vraiment besoin que $V$ converge exactement à $v_\\pi$ pour ensuite improve la policy ?\n",
    "\n",
    "Le Modified Policy Iteration (MPI) consiste à ne répéter seulement $k$ fois la mise à jour de $V(s)$, au lieu de la répéter jusqu'à que $V$ converge (ce que fait Policy Iteration). En effet, il est raisonnable de penser que seulement $k$ mises à jour suffisent à déterminer les meilleurs actions pour la policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Modified Policy Iteration Pseudocode</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat, while $V$ stops updating:\n",
    "\n",
    "&emsp;<strong>Evaluation</strong><br/>\n",
    "&emsp;repeat $k$ times:<br/>\n",
    "&emsp;&emsp; for each $s$:<br/>\n",
    "&emsp;&emsp;&emsp; $V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ V(s')]$\n",
    "\n",
    "&emsp;<strong>Improvement</strong><br/>\n",
    "&emsp;for each $s$:<br/>\n",
    "&emsp;&emsp;&emsp; $\\pi(s) \\leftarrow argmax_a [\\sum_{s'} p(s'|s,a) [R_{ss'}^a + γ V(s')]]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il se trouve que si $k=1$, alors cela revient au même que l'algorithme de Value Iteration, le second algorithme très utilisé pour résoudre un problème model-based."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
